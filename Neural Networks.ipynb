{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2d8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 22s 132ms/step - loss: 2.2961 - accuracy: 0.1514 - val_loss: 2.2827 - val_accuracy: 0.2100\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 2s 97ms/step - loss: 2.1441 - accuracy: 0.2816 - val_loss: 2.0346 - val_accuracy: 0.3100\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 2s 84ms/step - loss: 1.8237 - accuracy: 0.3129 - val_loss: 1.7759 - val_accuracy: 0.2400\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 2s 86ms/step - loss: 1.5686 - accuracy: 0.4443 - val_loss: 1.6856 - val_accuracy: 0.4300\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 2s 85ms/step - loss: 1.3294 - accuracy: 0.5544 - val_loss: 1.4507 - val_accuracy: 0.4500\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 2s 89ms/step - loss: 1.1584 - accuracy: 0.6483 - val_loss: 1.3039 - val_accuracy: 0.5550\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 2s 89ms/step - loss: 0.9302 - accuracy: 0.7760 - val_loss: 1.2742 - val_accuracy: 0.5250\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 2s 88ms/step - loss: 0.8537 - accuracy: 0.7872 - val_loss: 1.1298 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 2s 84ms/step - loss: 0.6503 - accuracy: 0.8536 - val_loss: 0.9886 - val_accuracy: 0.6800\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 2s 88ms/step - loss: 0.5323 - accuracy: 0.8836 - val_loss: 0.9113 - val_accuracy: 0.6950\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.9113 - accuracy: 0.6950\n",
      "Accuracy: 69.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read data from Excel file using pandas\n",
    "data = pd.read_excel('D:/Document Classification/Preprocessed data.xlsx')  # Replace 'your_file.xlsx' with the actual file path\n",
    "\n",
    "texts = data['Text']  # Assuming the text data is in a column named 'text_column'\n",
    "labels = data['Category']  # Assuming the labels/categories are in a column named 'label_column'\n",
    "\n",
    "# Convert categorical labels to numerical values using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenizing and padding sequences\n",
    "max_words = 1000\n",
    "maxlen = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(np.max(labels_encoded) + 1, activation='softmax'))  # Output layer for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b7d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 2s 30ms/step - loss: 2.2982 - accuracy: 0.1289 - val_loss: 2.2974 - val_accuracy: 0.1300\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 2.2253 - accuracy: 0.3104 - val_loss: 2.2684 - val_accuracy: 0.2350\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 2.1205 - accuracy: 0.4906 - val_loss: 2.1998 - val_accuracy: 0.2850\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.9157 - accuracy: 0.6496 - val_loss: 1.9985 - val_accuracy: 0.3550\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.5598 - accuracy: 0.7234 - val_loss: 1.7036 - val_accuracy: 0.4150\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.0947 - accuracy: 0.8473 - val_loss: 1.3087 - val_accuracy: 0.6550\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.6610 - accuracy: 0.9387 - val_loss: 1.0185 - val_accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.3608 - accuracy: 0.9750 - val_loss: 0.8763 - val_accuracy: 0.7300\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.1940 - accuracy: 0.9900 - val_loss: 0.7796 - val_accuracy: 0.7800\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.1118 - accuracy: 0.9975 - val_loss: 0.7375 - val_accuracy: 0.7750\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7375 - accuracy: 0.7750\n",
      "Accuracy: 77.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read data from Excel file using pandas\n",
    "data = pd.read_excel('D:/Document Classification/Preprocessed data.xlsx')  # Replace 'your_file.xlsx' with the actual file path\n",
    "\n",
    "texts = data['Text']  # Assuming the text data is in a column named 'text_column'\n",
    "labels = data['Category']  # Assuming the labels/categories are in a column named 'label_column'\n",
    "\n",
    "# Convert categorical labels to numerical values using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenizing and padding sequences\n",
    "max_words = 1000\n",
    "maxlen = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "cnn_model.add(Conv1D(32, 7, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(32, 7, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(np.max(labels_encoded) + 1, activation='softmax'))  # Output layer for multi-class classification\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532dc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
