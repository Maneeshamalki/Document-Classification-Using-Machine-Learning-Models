{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4RPRyRNWxtX",
    "outputId": "28fab066-039e-4af9-b20b-1b89305c4ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zrhk6I2XhQQ",
    "outputId": "d1748698-cfa8-4ae7-fef6-ac90809b6fec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z32hYvOGWfM6",
    "outputId": "cea4113c-bed5-46b7-cc90-d16d44c0f424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Category\n",
      "0  Lufthansa flies back to profit.German airline ...  business\n",
      "1  Japanese growth grinds to a halt\\n\\nGrowth in ...  business\n",
      "2  WorldCom director admits lying\\n\\nThe former c...  business\n",
      "3  Glaxo aims high after profit fall\\n\\nGlaxoSmit...  business\n",
      "4  Peugeot deal boosts Mitsubishi\\n\\nStruggling J...  business\n",
      "Number of Total Documents Before: 999\n",
      "Number of Duplicates Before: 19\n",
      "Number of Total Documents After: 980\n",
      "Number of Duplicates After: 0\n",
      "Accuracy: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        18\n",
      "           1       0.92      0.86      0.89        14\n",
      "           2       0.94      0.85      0.89        20\n",
      "           3       0.89      0.92      0.91        26\n",
      "           4       1.00      1.00      1.00        12\n",
      "           5       0.92      1.00      0.96        23\n",
      "           6       1.00      1.00      1.00        20\n",
      "           7       1.00      1.00      1.00        20\n",
      "           8       0.90      0.90      0.90        21\n",
      "           9       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.93       196\n",
      "   macro avg       0.94      0.94      0.94       196\n",
      "weighted avg       0.93      0.93      0.93       196\n",
      "\n",
      "\n",
      "Number of documents in each content category:\n",
      "graphics         100\n",
      "space            100\n",
      "business          99\n",
      "technologie       99\n",
      "food              98\n",
      "politics          98\n",
      "entertainment     97\n",
      "historical        97\n",
      "medical           96\n",
      "sport             96\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Excel data\n",
    "file_path = '/content/drive/MyDrive/SLT/Preprocessed data.xlsx'  # Replace with your file path\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(data.head())\n",
    "\n",
    "# Mapping categories to numerical labels\n",
    "content_to_label = {\n",
    "    'sport': 0,\n",
    "    'technologie': 1,\n",
    "    'business': 2,\n",
    "    'graphics': 3,\n",
    "    'entertainment': 4,\n",
    "    'politics': 5,\n",
    "    'food' :6,\n",
    "    'historical' :7,\n",
    "    'medical' :8,\n",
    "    'space' :9\n",
    "}\n",
    "\n",
    "# Convert content categories to numerical labels\n",
    "data['label'] = data['Category'].map(content_to_label)\n",
    "\n",
    "# Handling missing values in the label column\n",
    "data.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and lowercase\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", str(text)).lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Text' column\n",
    "data['Processed_Text'] = data['Text'].apply(preprocess_text)\n",
    "\n",
    "# Print the number of total documents before dropping\n",
    "total_docs_before = len(data)\n",
    "print(f\"Number of Total Documents Before: {total_docs_before}\")\n",
    "\n",
    "# Print the number of duplicates before dropping\n",
    "num_duplicates_before = data.duplicated(subset='Processed_Text').sum()\n",
    "print(f\"Number of Duplicates Before: {num_duplicates_before}\")\n",
    "\n",
    "# Drop duplicates based on the 'Processed_Text' column\n",
    "data = data.drop_duplicates(subset='Processed_Text', keep='first')\n",
    "\n",
    "# Print the number of total documents after dropping\n",
    "total_docs_after = len(data)\n",
    "print(f\"Number of Total Documents After: {total_docs_after}\")\n",
    "\n",
    "# Print the number of duplicates after dropping\n",
    "num_duplicates_after = data.duplicated(subset='Processed_Text').sum()\n",
    "print(f\"Number of Duplicates After: {num_duplicates_after}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Processed_Text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Find the number of documents in each content category\n",
    "content_counts = data['Category'].value_counts()\n",
    "print(\"\\nNumber of documents in each content category:\")\n",
    "print(content_counts)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
